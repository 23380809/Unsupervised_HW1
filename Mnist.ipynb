{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "def parse_mnist_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num_images, num_rows, num_cols = struct.unpack('>IIII', f.read(16))\n",
    "\n",
    "        if magic != 2051:\n",
    "            raise ValueError(\"Invalid magic number in the MNIST image file\")\n",
    "\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, num_rows, num_cols)\n",
    "\n",
    "    return images\n",
    "\n",
    "def parse_mnist_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "\n",
    "        if magic != 2049:\n",
    "            raise ValueError(\"Invalid magic number in the MNIST label file\")\n",
    "\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    return labels\n",
    "\n",
    "images_path = 'train-images-idx3-ubyte'\n",
    "labels_path = 'train-labels-idx1-ubyte'\n",
    "\n",
    "images = parse_mnist_images(images_path)\n",
    "images = images[:1000]\n",
    "labels = parse_mnist_labels(labels_path)\n",
    "labels = labels[:1000]\n",
    "\n",
    "ng = fetch_20newsgroups(subset='train') \n",
    "documents = ng.data\n",
    "labels = ng.target\n",
    "documents = documents[:1000]\n",
    "labels = labels[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: (1000, 784)\n",
      "documents shape: (1000, 32190)\n",
      "0.0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def shift_scale_normalize(images):\n",
    "    flat_images = images.reshape(images.shape[0], -1)\n",
    "    min_val = np.min(flat_images)\n",
    "    max_val = np.max(flat_images)\n",
    "    images_normalized = (flat_images - min_val) / (max_val - min_val)\n",
    "\n",
    "    images_normalized = images_normalized.reshape(images.shape)\n",
    "    \n",
    "    return images_normalized\n",
    "\n",
    "\n",
    "def zero_mean_normalize(images):\n",
    "    flat_images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "    # zero mean normalization formula is (x - mean) / std_dev\n",
    "    images_normalized = (flat_images - np.mean(flat_images)) / np.std(flat_images)\n",
    "\n",
    "    images_normalized = images_normalized.reshape(images.shape)\n",
    "    \n",
    "    return images_normalized\n",
    "\n",
    "\n",
    "## by calculating the term frequency, we can get the term frequency matrix\n",
    "def term_frequency_weighting_normalize(documents):\n",
    "    # stop words! remove common english words\n",
    "    vectorizer = TfidfVectorizer(norm=None, use_idf=False)\n",
    "    matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "normalized_documents = term_frequency_weighting_normalize(documents).toarray()\n",
    "images = shift_scale_normalize(images)\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "print(\"images shape:\", images.shape)\n",
    "print(\"documents shape:\", normalized_documents.shape)\n",
    "print(images[0][10])\n",
    "print(normalized_documents[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Euclidian distance using Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          9.36122213 10.87509456 10.01890101 10.48026358 10.12663683\n",
      "  9.5042814   8.50966658  9.31437108 10.07571719]\n",
      "[ 0.         18.27566688 27.60434748 17.17556404 18.81488772 38.27531842\n",
      " 16.24807681 46.3788745  15.32970972 25.43619468]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_euclidean_distances(dataset_1, dataset_2):\n",
    "\n",
    "    distances = cdist(dataset_1, dataset_2, metric='euclidean')\n",
    "\n",
    "    return distances\n",
    "\n",
    "images_distances = compute_euclidean_distances(images, images)\n",
    "documents_distances = compute_euclidean_distances(normalized_documents, normalized_documents)\n",
    "\n",
    "print(images_distances[0][:10])\n",
    "print(documents_distances[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Euclidian distance using my own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def compute_euclidean_distances_custom(dataset_1, dataset_2):\n",
    "    \n",
    "    num_dataset_1 = dataset_1.shape[0]\n",
    "    num_dataset_2 = dataset_2.shape[0]\n",
    "    \n",
    "    # Initialize a matrix\n",
    "    distances = np.zeros((num_dataset_1, num_dataset_2))\n",
    "\n",
    "    for i in range(num_dataset_1):\n",
    "        for j in range(num_dataset_2):\n",
    "            dist = np.sum((dataset_1[i] - dataset_2[j])**2)\n",
    "            distances[i, j] = np.sqrt(dist)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# images_distances = compute_euclidean_distances_custom(images, images)\n",
    "# documents_distances = compute_euclidean_distances_custom(normalized_documents, normalized_documents)\n",
    "\n",
    "# print(images_distances[0])\n",
    "# print(documents_distances[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Cosine Similarity using my own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(dataset_1, dataset_2):\n",
    "\n",
    "    # Compute the dot product between the two sets of images\n",
    "    dot_product = np.dot(dataset_1, dataset_2.T)\n",
    "    \n",
    "    # Calculate the norm of each image in both sets\n",
    "    norm_1 = np.linalg.norm(dataset_1, axis=1)\n",
    "    norm_2 = np.linalg.norm(dataset_2, axis=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / np.outer(norm_1, norm_2)\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "# images_cosine_similarity = compute_cosine_similarity(images, images)\n",
    "# documents_cosine_similarity = compute_cosine_similarity(normalized_documents, normalized_documents)\n",
    "\n",
    "# print(images_cosine_similarity[0])\n",
    "# print(documents_cosine_similarity[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - Define KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict_euclidean_distances(self, X_test):\n",
    "        predictions = []\n",
    "        distances = compute_euclidean_distances(X_test, self.X_train)\n",
    "\n",
    "        for dist in distances:\n",
    "            k_nearest_i = np.argsort(dist)[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_nearest_i]\n",
    "\n",
    "            predictions.append(max(set(k_nearest_labels), key=k_nearest_labels.count))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_cosine_similarity(self, X_test):\n",
    "        predictions = []\n",
    "        cosine_similarities = compute_cosine_similarity(X_test, self.X_train)\n",
    "\n",
    "        for sim in cosine_similarities:\n",
    "            k_nearest_i = np.argsort(sim)[::-1][:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_nearest_i]\n",
    "            \n",
    "            predictions.append(max(set(k_nearest_labels), key=k_nearest_labels.count))\n",
    "\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.sum(y_true == y_pred) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - test data - mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "Test Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images_path = 'train-images-idx3-ubyte'\n",
    "labels_path = 'train-labels-idx1-ubyte'\n",
    "\n",
    "images = parse_mnist_images(images_path)\n",
    "labels = parse_mnist_labels(labels_path)\n",
    "images = images[:1000]\n",
    "labels = labels[:1000]\n",
    "\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "images = shift_scale_normalize(images)\n",
    "\n",
    "split_train = int(len(images) * 0.8)\n",
    "split_valid = int(len(images) * 0.9)\n",
    "\n",
    "X_train, y_train = images[:split_train], labels[:split_train]\n",
    "X_valid, y_valid = images[split_train:split_valid], labels[split_train:split_valid]\n",
    "X_test, y_test = images[split_valid:], labels[split_valid:]\n",
    "\n",
    "knn = KNNClassifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_valid_pred = knn.predict_euclidean_distances(X_valid)\n",
    "validation_accuracy = knn.accuracy(y_valid, y_valid_pred)\n",
    "print(f'Validation Accuracy: {validation_accuracy:.2f}')\n",
    "\n",
    "y_test_pred = knn.predict_euclidean_distances(X_test)\n",
    "test_accuracy = knn.accuracy(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - test data - 20 newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.59\n",
      "Test Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "ng = fetch_20newsgroups(subset='train')\n",
    "documents = ng.data\n",
    "labels = ng.target\n",
    "documents = documents\n",
    "labels = labels\n",
    "\n",
    "normalized_documents = term_frequency_weighting_normalize(documents).toarray()\n",
    "\n",
    "split_train = int(len(normalized_documents) * 0.8)\n",
    "split_valid = int(len(normalized_documents) * 0.9)\n",
    "\n",
    "X_train, y_train = normalized_documents[:split_train], labels[:split_train]\n",
    "X_valid, y_valid = normalized_documents[split_train:split_valid], labels[split_train:split_valid]\n",
    "X_test, y_test = normalized_documents[split_valid:], labels[split_valid:]\n",
    "\n",
    "knn = KNNClassifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_valid_pred = knn.predict_cosine_similarity(X_valid)\n",
    "validation_accuracy = knn.accuracy(y_valid, y_valid_pred)\n",
    "print(f'Validation Accuracy: {validation_accuracy:.2f}')\n",
    "\n",
    "y_test_pred = knn.predict_cosine_similarity(X_test)\n",
    "test_accuracy = knn.accuracy(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "\n",
    "# y_valid_pred = knn.predict_euclidean_distances(X_valid)\n",
    "# validation_accuracy = knn.accuracy(y_valid, y_valid_pred)\n",
    "# print(f'Validation Accuracy: {validation_accuracy:.2f}')\n",
    "\n",
    "# y_test_pred = knn.predict_euclidean_distances(X_test)\n",
    "# test_accuracy = knn.accuracy(y_test, y_test_pred)\n",
    "# print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
